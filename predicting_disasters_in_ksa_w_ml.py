# -*- coding: utf-8 -*-
"""Predicting disasters in KSA w ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-l3Zo0Vv5g6LLYcLLoTzskkyfwhR37tN

#**Predicting Deadly Disasters in Saudi Arabia Using Machine Learning**

# 1. Introduction

This notebook builds two classification models based on disaster data in Saudi Arabia:

Predict if a disaster is deadly (Is_Deadly = 1 if at least one death occurred)

Predict the severity level (Low / Medium / High) based on estimated damage

These models help support early warning and decision-making in crisis response.
We applied Logistic Regression and Random Forest to assess model performance.



---

# 2. Dataset Description
The dataset contains records of disasters with:

Type, region, year, and country

Magnitude and reported deaths

Estimated damages

Two target variables were created:

Is_Deadly: binary, based on Total Deaths

Severity_Level: categorical, based on Total Damage


---

# 3.Library Imports and Data Loading
We will load the disaster dataset (after cleaning) first and then perform necessary imports.
"""

!pip install -q imbalanced-learn

#imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from imblearn.over_sampling import SMOTE

sns.set(style="whitegrid")

df = pd.read_csv("merged_output.csv")

print("Dataset Loaded!")
print("\n- Shape:", df.shape)
print("\n- Columns:\n", df.columns.tolist())
print("\n- Info:")
df.info()

df.head()

"""#4. Data Preprocessing
##We perform the following preprocessing steps:

One-hot Encoding: Converts categorical variables (like disaster type and region) into numerical format suitable for ML models.

Target Creation: The Is_Deadly column is created based on the Total Deaths.

Feature Selection: Irrelevant or leakage-prone columns are excluded.

Balancing the Dataset: Since most disasters are non-deadly, the dataset is imbalanced. We apply SMOTE (Synthetic Minority Oversampling Technique) to balance the classes.

Train-Test Split: The balanced dataset is split into training and testing sets (80/20 split).

Scaling: Features are standardized to have zero mean and unit variance.
"""

# Create target: Is_Deadly (binary)
df["Is_Deadly"] = df["Total Deaths"].apply(lambda x: 1 if x > 0 else 0)

# Create Severity Level based on Total Damage
def classify_severity(damage):
    if damage < 100000:
        return "Low"
    elif damage < 1000000:
        return "Medium"
    else:
        return "High"

df["Severity_Level"] = df["Total Damage ('000 US$)"].apply(classify_severity)

# List of irrelevant columns to drop
columns_to_drop = [
    'DisNo.',
    'ISO',
    'Entry Date',
    'Last Update',
    "Reconstruction Costs ('000 US$)",
    "Reconstruction Costs, Adjusted ('000 US$)",
    "Insured Damage ('000 US$)",
    "Insured Damage, Adjusted ('000 US$)",
    "Total Damage, Adjusted ('000 US$)",
    'CPI'
]

# Drop only existing columns to avoid errors
safe_to_drop = [col for col in columns_to_drop if col in df.columns]
df.drop(columns=safe_to_drop, inplace=True)

# Show results
print("Dropped columns (if existed):", safe_to_drop)
print("Cleaned dataset shape:", df.shape)

# Preview the important columns
df[['Total Deaths', 'Is_Deadly', "Total Damage ('000 US$)", 'Severity_Level']].head()

"""#5. Is_Deadly Classification
Goal: Predict whether a disaster causes death or not.

Steps:

Created binary label from Total Deaths

Trained Logistic Regression and Random Forest

Evaluated using accuracy and confusion matrix


"""

# Class balance for Is_Deadly
plt.figure(figsize=(6, 4))
sns.countplot(x='Is_Deadly', data=df, palette='Set2')
plt.title('Class Distribution: Is_Deadly')
plt.xticks([0, 1], ['Not Deadly', 'Deadly'])
plt.xlabel('Disaster Outcome')
plt.ylabel('Count')
plt.show()

"""#6. Severity Level Classification
Goal: Predict damage severity: Low, Medium, or High.

Steps:

Converted numeric damage into 3 bins

Reused encoded features

Trained both models again

Evaluated using classification report and confusion matrix


"""

# Class balance for Severity_Level
plt.figure(figsize=(6, 4))
sns.countplot(x='Severity_Level', data=df, order=['Low', 'Medium', 'High'], palette='Set1')
plt.title('Severity Level Distribution')
plt.xlabel('Severity')
plt.ylabel('Count')
plt.show()

# Most common disaster types
plt.figure(figsize=(10, 4))
sns.countplot(y='Disaster Type', data=df, order=df['Disaster Type'].value_counts().index, palette='coolwarm')
plt.title('Top Disaster Types')
plt.xlabel('Count')
plt.ylabel('Disaster Type')
plt.tight_layout()
plt.show()

"""#7.Predicting Is_Deadly
Goal: Predict whether a disaster caused at least one death.

Steps:

Created binary label from Total Deaths

Selected only pre-outcome features

Used one-hot encoding

Balanced dataset with SMOTE

Trained Logistic Regression and Random Forest

Evaluated using classification report and confusion matrix


"""

# Select features that are known BEFORE knowing the outcome
features = [
    'Disaster Type', 'Region', 'Country',
    'Start Year', 'Start Month',
    'Magnitude', 'Magnitude Scale'
]

# Target: Is_Deadly
target = 'Is_Deadly'

# Subset the data
df_model = df[features + [target]].copy()

# One-hot encode categorical features
df_model = pd.get_dummies(df_model, columns=['Disaster Type', 'Region', 'Country', 'Magnitude Scale'], drop_first=True)

# Split X and y
X = df_model.drop(target, axis=1)
y = df_model[target]

# Balance the classes using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

print("Feature engineering complete!")
print(" X_train shape:", X_train.shape)
print(" y_train distribution:\n", pd.Series(y_train).value_counts())

# Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
log_preds = logreg.predict(X_test)

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

# Evaluation
def evaluate_model(name, y_true, y_pred):
    print(f"\n {name} Accuracy: {accuracy_score(y_true, y_pred):.2%}")
    print(classification_report(y_true, y_pred, target_names=["Not Deadly", "Deadly"]))

# Evaluate both
evaluate_model("Logistic Regression", y_test, log_preds)
evaluate_model("Random Forest", y_test, rf_preds)

# Confusion Matrix for Random Forest
cm = confusion_matrix(y_test, rf_preds)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Deadly", "Deadly"], yticklabels=["Not Deadly", "Deadly"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix: Random Forest")
plt.show()

# Rebuild the same features on the full dataset (no train/test split)
df_full = df.copy()

# One-hot encode just like before
df_full_model = pd.get_dummies(
    df_full[features],
    columns=['Disaster Type', 'Region', 'Country', 'Magnitude Scale'],
    drop_first=True
)

# Ensure the encoded columns match the modelâ€™s training features
missing_cols = set(X.columns) - set(df_full_model.columns)
for col in missing_cols:
    df_full_model[col] = 0
df_full_model = df_full_model[X.columns]  # Reorder to match model

# Scale features
df_full_scaled = scaler.transform(df_full_model)

# Predict using trained Logistic Regression model instead of RF
df['Predicted_Is_Deadly'] = logreg.predict(df_full_scaled)

# Save to CSV
df.to_csv("disaster_predictions_logreg.csv", index=False)
print("Saved Logistic Regression predictions to 'disaster_predictions_logreg.csv'")

# Evaluate on training set
train_preds = rf.predict(X_train)
train_acc = accuracy_score(y_train, train_preds)

# Evaluate on test set (already done earlier)
test_preds = rf.predict(X_test)
test_acc = accuracy_score(y_test, test_preds)

print(f" Training Accuracy: {train_acc:.2%}")
print(f" Test Accuracy: {test_acc:.2%}")

"""#8.Predicting Severity_Level
Goal: Classify disaster into severity levels based on damage.

Steps:

Converted numeric damage to 3-class label

Used same encoded features

Trained both Logistic Regression and Random Forest

Evaluated results using accuracy, classification report, confusion matrix
"""

from sklearn.preprocessing import LabelEncoder

# Set features (same as before)
severity_features = [
    'Disaster Type', 'Region', 'Country',
    'Start Year', 'Start Month',
    'Magnitude', 'Magnitude Scale'
]

target = 'Severity_Level'

# Subset and clean
df_sev = df[severity_features + [target]].copy()

# Encode target labels (Low=0, Medium=1, High=2)
label_encoder = LabelEncoder()
df_sev[target] = label_encoder.fit_transform(df_sev[target])

# One-hot encode categorical features
df_sev = pd.get_dummies(df_sev, columns=['Disaster Type', 'Region', 'Country', 'Magnitude Scale'], drop_first=True)

# Split X and y
X_sev = df_sev.drop(target, axis=1)
y_sev = df_sev[target]

# Optionally balance with SMOTE
smote = SMOTE(random_state=42)
X_resampled_sev, y_resampled_sev = smote.fit_resample(X_sev, y_sev)

# Scale features
X_rescaled_sev = scaler.fit_transform(X_resampled_sev)

# Train/Test split
X_train_sev, X_test_sev, y_train_sev, y_test_sev = train_test_split(X_rescaled_sev, y_resampled_sev, test_size=0.2, random_state=42)

print("Severity data prepared!")
print("X_train shape:", X_train_sev.shape)
print("y_train value counts:\n", pd.Series(y_train_sev).value_counts())

from sklearn.metrics import classification_report, confusion_matrix

# Logistic Regression
logreg_sev = LogisticRegression(max_iter=1000)
logreg_sev.fit(X_train_sev, y_train_sev)
log_preds_sev = logreg_sev.predict(X_test_sev)

# Random Forest
rf_sev = RandomForestClassifier(random_state=42)
rf_sev.fit(X_train_sev, y_train_sev)
rf_preds_sev = rf_sev.predict(X_test_sev)

# Evaluation function
def eval_model(name, y_true, y_pred):
    print(f"\n{name} Accuracy: {accuracy_score(y_true, y_pred):.2%}")
    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

# Evaluate both models
eval_model("Logistic Regression", y_test_sev, log_preds_sev)
eval_model("Random Forest", y_test_sev, rf_preds_sev)

# Confusion Matrix for Random Forest
cm = confusion_matrix(y_test_sev, rf_preds_sev)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap="YlGnBu",
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix: Severity Level (Random Forest)")
plt.show()

# Prepare features from the full dataset
df_sev_full = df.copy()

# One-hot encode as before
df_sev_full_model = pd.get_dummies(
    df_sev_full[severity_features],
    columns=['Disaster Type', 'Region', 'Country', 'Magnitude Scale'],
    drop_first=True
)

# Match column structure to training features
missing_cols = set(X_sev.columns) - set(df_sev_full_model.columns)
for col in missing_cols:
    df_sev_full_model[col] = 0
df_sev_full_model = df_sev_full_model[X_sev.columns]

# Scale using the same scaler
df_sev_full_scaled = scaler.transform(df_sev_full_model)

# Predict with Random Forest
sev_preds = rf_sev.predict(df_sev_full_scaled)

# Decode back to original labels (Low/Medium/High)
df["Predicted_Severity_Level"] = label_encoder.inverse_transform(sev_preds)

# Save the enriched dataset
df.to_csv("disaster_predictions_with_severity.csv", index=False)
print("Saved predictions to 'disaster_predictions_with_severity.csv'")

"""#9.Feature Importance
Used feature_importances_ from Random Forest to plot top contributing features

Separate importance graphs were made for:

Is_Deadly prediction

Severity_Level prediction

These visuals help explain what drives the model predictions.


"""

# Get feature importances from the trained Random Forest
#This shows which features most influenced Severity_Level predictions.
importances = rf_sev.feature_importances_
feature_names = X_sev.columns

# Combine into a DataFrame
feat_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot top 10
plt.figure(figsize=(10, 6))
sns.barplot(data=feat_df.head(10), x='Importance', y='Feature', palette="viridis")
plt.title("Top 10 Important Features for Severity Level Prediction")
plt.xlabel("Feature Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#10. Use Cases
Support for early warning systems

Helps emergency response teams prioritize deadly or high-damage events

Can be integrated into disaster risk dashboards

#11.Limitations
Damage estimates and death counts may not be fully reliable

Historical patterns may not reflect future disasters (e.g. climate change)

More context features (e.g. population density, weather) would improve the model
"""